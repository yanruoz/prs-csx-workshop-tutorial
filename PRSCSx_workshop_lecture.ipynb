{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c5478d2",
   "metadata": {},
   "source": [
    "# Overall flow:\n",
    "1. **Run PRS-CSx**\n",
    "2. **Prepare matrices for running regression**\n",
    "3. **Run regression to find weight parameters a_hat and b_hat (y = a_hat * X @ Weas + b_hat * X @ Weur)**\n",
    "4. **Predict phenotype on validation and test dataset**\n",
    "5. **Plot true values against predicted values**\n",
    "6. **Evaluate using deviance-based R^2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bed196",
   "metadata": {},
   "source": [
    "**This Jupyter Notebook is the lecture version of the PRS-CSx workshop tutorial. It instructs on how to train the model to find weight parameters to assign to each population in phenotype prediction and how to evaluate the model using deviance-based R^2.**\n",
    "\n",
    "Disclaimer: \n",
    "<br>\n",
    "This is just one way of coding. Please feel free to experiment on your self, and you're welcome to share if you find ways of coding more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e31bd7",
   "metadata": {},
   "source": [
    "# 1. Run PRS-CSx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef37c4",
   "metadata": {},
   "source": [
    "## Setting Up PRS-CSx\n",
    "\n",
    "- **0. Create a folder to be your working directory. Let's name it `user_test` for the purpose of subsequent explanations.**\n",
    "    <br>\n",
    "\n",
    "- **1. Clone the PRS-CSx repository using the following git command**:\n",
    "    ```bash\n",
    "    git clone https://github.com/getian107/PRScsx.git\n",
    "    ```\n",
    "\n",
    "    Alternatively, download the source files from the [GitHub website](https://github.com/getian107/PRScsx) to `user_test`.\n",
    "    <br>\n",
    "\n",
    "- **2. In `user_test`, create a sub-folder named `ref`. Download the LD reference panels to `ref` and extract files**:\n",
    "    LD reference panels constructed using the 1000 Genomes Project phase 3 samples:\n",
    "\n",
    "    - EAS reference (~4.33G): \n",
    "        ```bash\n",
    "        wget https://www.dropbox.com/s/7ek4lwwf2b7f749/ldblk_1kg_eas.tar.gz?dl=0\n",
    "        tar -zxvf ldblk_1kg_eas.tar.gz\n",
    "        ```\n",
    "\n",
    "    - EUR reference (~4.56G): \n",
    "        ```bash\n",
    "        wget https://www.dropbox.com/s/mt6var0z96vb6fv/ldblk_1kg_eur.tar.gz?e=1&dl=0\n",
    "        tar -zxvf ldblk_1kg_eur.tar.gz\n",
    "        ```\n",
    "\n",
    "    Note that these files are identical to the reference panels used in PRS-CS. Therefore, there is no need to download again if you are already using PRS-CS.\n",
    "\n",
    "    For regions that don't have access to Dropbox, reference panels can be downloaded from the alternative download site.\n",
    "    <br>\n",
    "\n",
    "- **3. Download the SNP information file and put it in the same folder containing the reference panels**:\n",
    "    - 1000 Genomes reference: SNP info (~106M): \n",
    "        ```bash\n",
    "        wget https://www.dropbox.com/s/rhi806sstvppzzz/snpinfo_mult_1kg_hm3?dl=0\n",
    "        ```\n",
    "    <br>\n",
    "\n",
    "- **4. PRScsx requires Python packages `scipy` and `h5py` installed**:\n",
    "    - [scipy](https://www.scipy.org/)\n",
    "    - [h5py](https://www.h5py.org/)\n",
    "    <br>\n",
    "\n",
    "- **5. Once Python and its dependencies have been installed, running the following will print a list of command-line options.**:\n",
    "    ```bash\n",
    "    ./PRScsx.py --help \n",
    "    # or \n",
    "    ./PRScsx.py -h\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713ec843",
   "metadata": {},
   "source": [
    "## Using PRS-CSx with Test Data\n",
    "The test data contains EUR and EAS GWAS summary statistics and a bim file for 1,000 SNPs on chromosome 22. An example to use the test data:\n",
    "```bash\n",
    "python PRScsx.py --ref_dir=path_to_ref --bim_prefix=path_to_bim/test --sst_file=path_to_sumstats/EUR_sumstats.txt,path_to_sumstats/EAS_sumstats.txt --n_gwas=200000,100000 --pop=EUR,EAS --chrom=22 --phi=1e-2 --out_dir=path_to_output --out_name=test\n",
    "```\n",
    "The test data analysis would be finished in approximately 1 min when using 8Gb of RAM.\n",
    "\n",
    "Example use given that the reference panels are downloaded in a folder named `ref` and that the current working directory is where PRScsx.py is located:\n",
    "1. Create a directory to store output:\n",
    "    ```bash\n",
    "    mkdir -p ../output\n",
    "    ```\n",
    "    \n",
    "2. Run PRS-CSx: <br>\n",
    "    ```bash\n",
    "    python PRScsx.py --ref_dir=../ref --bim_prefix=./test_data/test --sst_file=./test_data/EUR_sumstats.txt,./test_data/EAS_sumstats.txt --n_gwas=200000,100000 --pop=EUR,EAS --chrom=22 --phi=1e-2 --out_dir=../output --out_name=test\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4dd43b",
   "metadata": {},
   "source": [
    "# 2. Prepare Matrices for Running Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be98c1",
   "metadata": {},
   "source": [
    "### Import Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18c35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390bcbf",
   "metadata": {},
   "source": [
    "### Set working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set current working directory to the user_test folder\n",
    "# os.chdir(\"path_to_user_test/user_test\")\n",
    "os.chdir(\"/Users/aliceyan/Desktop/huang_lab_yanruoz/wcpg2023/user_test\") \n",
    "\n",
    "# Check current working directory\n",
    "cwd = os.getcwd()\n",
    "print(f\"Current working directory: {cwd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47fe972",
   "metadata": {},
   "source": [
    "### Find overlapping variants and store the info of these variants from both populations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d39aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "path_EAS_var_w = cwd + '/output/EAS_var_w.txt'\n",
    "path_EUR_var_w = cwd + '/output/EUR_var_w.txt'\n",
    "\n",
    "# Read in the files and put them in dictionaries\n",
    "EAS_var_w = pd.read_csv(path_EAS_var_w, sep = '\\t', header = None) \n",
    "EUR_var_w = pd.read_csv(path_EUR_var_w, sep = '\\t', header = None)\n",
    "\n",
    "EAS_dic = dict(zip(EAS_var_w[0], EAS_var_w[1])) \n",
    "EUR_dic = dict(zip(EAS_var_w[0], EAS_var_w[1]))\n",
    "\n",
    "# Find overlapping rsid variants and create a new dictionary with these variants and a tuple of values from both dictionaries\n",
    "overlap_var = set(EAS_dic.keys()).intersection(set(EUR_dic.keys())) \n",
    "overlap_var_list = list(overlap_var) \n",
    "combined_dict = {key: (EAS_dic[key], EUR_dic[key]) for key in overlap_var} \n",
    "\n",
    "# Write the overlapping variants to a txt file\n",
    "path_overlap_risk_variants = cwd + '/overlap_risk_variants.txt'\n",
    "with open(path_overlap_risk_variants, 'w') as f: \n",
    "    # open a file in the specified path for writing mode; context manaer 'with' \n",
    "    # --> ensure the file is properly closed after writing\n",
    "    for key in overlap_var: \n",
    "    # iterate through the set of overlapping variants\n",
    "        f.write(f\"{key}\\n\") \n",
    "        # write out each variant followed by a new line character \"\\n\" --> each variant written on a new line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d0873",
   "metadata": {},
   "source": [
    "### Prepare the variant weights matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5491975",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.10f' % x) \n",
    "# display floating point numbers up to 10 decimal places; ensures that the numbers are shown with full precision\n",
    "\n",
    "# W_eas\n",
    "W_eas = EAS_var_w[EAS_var_w[0].isin(overlap_var_list)][[1]] \n",
    "# from the EAS_var_w df, select rows where the variants (column 0) are in the overlap_var_list, \n",
    "# and only retain the weight (column 1) in that row; double bracket: ensures that the result is df \n",
    "W_eas = W_eas.values \n",
    "# convert df to numpy array for later matrix multiplication\n",
    "\n",
    "# W_eur\n",
    "W_eur = EUR_var_w[EUR_var_w[0].isin(overlap_var_list)][[1]]\n",
    "W_eur = W_eur.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73522e26",
   "metadata": {},
   "source": [
    "### Prepare the genotype matrix X and phenotype matrix y in validation and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fbd2f1",
   "metadata": {},
   "source": [
    "Download the file with genotype (from 1kg) and phenotype (EAS population) info to `user_test` folder:\n",
    "```bash\n",
    "wget https://github.com/yanruoz/prs-csx-workshop-tutorial/blob/main/genotype_phenotype_1kgEAS.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16a4e048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.         0.         0.         ... 0.         1.         0.04249645]\n",
      " [2.         0.         1.         ... 0.         0.         0.08080722]\n",
      " [1.         0.         2.         ... 1.         2.         0.06038308]\n",
      " ...\n",
      " [0.         0.         1.         ... 0.         1.         0.03393009]\n",
      " [1.         0.         2.         ... 0.         0.         0.04689794]\n",
      " [1.         0.         2.         ... 0.         1.         0.03195786]]\n",
      "(504, 902)\n"
     ]
    }
   ],
   "source": [
    "# Read in the file that has genotype and phenotype for each individual\n",
    "path_data = cwd + \"/genotype_phenotype_1kgEAS.txt\"\n",
    "data = np.loadtxt(path_data)\n",
    "\n",
    "# The file is prepared such that it has 504 rows (individuals) x 902 columns (variants + effect sizes); \n",
    "# entries: first 901 columns: allele count for each variant; last column = simulated phenotype value for the individual\n",
    "print(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c797d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate genotype and phenotype information\n",
    "geno = data[:, :-1] # extract all the rows, and all the columns except the last one\n",
    "phen = data[:, -1] # extract all the rows, and only the last column\n",
    "\n",
    "# print(geno.shape)# geno is a 2D array (matrix) with 504 rows and 901 columns\n",
    "# print(geno) # output: (504, 901)\n",
    "\n",
    "# print(phen.shape) # phen is a 1D array with 504 elements\n",
    "# print(phen) # output: (504,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bfe964",
   "metadata": {},
   "source": [
    "### Split Validation and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860c7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(154) # set a random seed for reproducibility\n",
    "vali_proportion = 0.4 # specify the proportion of data to be used for validation\n",
    "vali_size = int(geno.shape[0] * vali_proportion) # calculate the number of samples based on the proportion\n",
    "\n",
    "vali_indices = np.random.choice(geno.shape[0], vali_size, replace = False) \n",
    "# randomly select a unique set of indices for the validation dataset\n",
    "# np.random.choice: method to generate a random sample from a given 1D array or integer range\n",
    "# geno.shape[0]: 504 --> range to sample from (rows of geno df = total number of individuals/samples)\n",
    "# vali_size: 202 --> how many samples to pick from the range\n",
    "# replace = False --> once the index is chosen, it can't be chosen again\n",
    "\n",
    "test_indices = np.setdiff1d(np.arange(geno.shape[0]), vali_indices)\n",
    "# np.setdiff1d(A, B): function that returns the sorted, unique values in array A that are not in array B\n",
    "# A = np.arange(geno.shape[0]): an array of consecutive int ranging from 0 to geno.shape[0]-1\n",
    "# B = vali_indices: an array that contains the indices selected for validation\n",
    "\n",
    "# Extract the rows according to the indices for geno, phen, vali, and test respectively\n",
    "X_vali = geno[vali_indices] \n",
    "y_vali = phen[vali_indices]\n",
    "\n",
    "X_test = geno[test_indices]\n",
    "y_test = phen[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b7676",
   "metadata": {},
   "source": [
    "# 3. Run Regression (to find weight parameters a_hat and b_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a7a75",
   "metadata": {},
   "source": [
    "### Prepare the model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea92a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "XWeas_vali = X_vali @ W_eas # based on the equation, calculate the weighted input data for the EAS population\n",
    "XWeur_vali = X_vali @ W_eur\n",
    "XW_vali = np.hstack((XWeas_vali, XWeur_vali)) \n",
    "# horizontally stack the weighted inputs for both population for model input\n",
    "\n",
    "# In essence, this block of code takes the validation data, \n",
    "# multiplies it by population-specific weight matrices, \n",
    "# and then concatenates the results side by side to form a combined input for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5cc6fe",
   "metadata": {},
   "source": [
    "### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c710920",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept = False).fit(XW_vali, y_vali) \n",
    "# we usually don’t include intercept in the PRS calculation. \n",
    "# As a result the PRS calculated in this manner only reflects the relative risk, not the absolute risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f84e61",
   "metadata": {},
   "source": [
    "### Obtain the regression parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "151c7a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_hat =0.9337471644122599\n",
      "b_hat =-0.08808776795046623\n"
     ]
    }
   ],
   "source": [
    "a_hat = model.coef_[0]\n",
    "b_hat = model.coef_[1]\n",
    "print(f\"{a_hat =}\")\n",
    "print(f\"{b_hat =}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b5ffc",
   "metadata": {},
   "source": [
    "# 4. Predict Phenotype on Validation and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad8cb44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 1)\n",
      "(201,)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on validation data\n",
    "y_hat_vali = a_hat * XWeas_vali + b_hat * XWeur_vali\n",
    "print(y_hat_vali.shape)\n",
    "y_hat_vali = y_hat_vali.flatten() #flatten(): method that flatten a 2D matrix into a 1D array\n",
    "print(y_hat_vali.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "550cd83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction on test data\n",
    "XWeas_test = X_test @ W_eas\n",
    "XWeur_test = X_test @ W_eur\n",
    "y_hat = a_hat * XWeas_test + b_hat * XWeur_test\n",
    "y_hat = y_hat.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a027a",
   "metadata": {},
   "source": [
    "# 5. Plot True values against Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b8719a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5)) # create a blank canvas 10 in. wide x 5 inch. tall\n",
    "\n",
    "# Compute global minimum and maximum for x and y axes for setting the x & y ranges\n",
    "min_true = min(min(y_vali), min(y_test))\n",
    "max_true = max(max(y_vali), max(y_test))\n",
    "min_pred = min(min(y_hat_vali), min(y_hat))\n",
    "max_pred = max(max(y_hat_vali), max(y_hat))\n",
    "\n",
    "# For the validation cohort\n",
    "plt.subplot(1, 2, 1) # (# rows, # columns, index of this plot)\n",
    "plt.scatter(y_vali, y_hat_vali, alpha = 0.5) # alpha sets the transparency of the dot markers\n",
    "plt.title('Validation Dataset')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.plot([min_true, max_true], [min_pred, max_pred], 'k', linestyle='dashed') \n",
    "# draw a line that shows perfect correlation between true and predicted values\n",
    "\n",
    "# For the test cohort\n",
    "plt.subplot(1, 2, 2) \n",
    "plt.scatter(y_test, y_hat, alpha = 0.5)\n",
    "plt.title('Test Dataset')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.plot([min_true, max_true], [min_pred, max_pred], 'k', linestyle='dashed')\n",
    "\n",
    "# Save the figure & inspect it!\n",
    "plt.savefig('/home/scur0497/test/test_output/true_against_pred.png') \n",
    "# if you don't specify the directory, it will be where this script is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea814ddf",
   "metadata": {},
   "source": [
    "# 6. Evaluate Using Deviance-based R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef99d64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deviance = 0.09691952219045474\n"
     ]
    }
   ],
   "source": [
    "# Calculate the deviance based on the y_hat for the test data \n",
    "# (= residual deviance = total squared difference between the true output values and the predicted output values)\n",
    "deviance = mean_squared_error(y_test, y_hat, squared = True) * len(y_test)\n",
    "# mean_squared_error(): calculates the mean squared error (MSE) between true and predicted values, \n",
    "# which is the average squared differences between true and predicted values\n",
    "\n",
    "# multiplying it with len(y_test) gives the total squared difference\n",
    "print(f\"{deviance = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36047996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deviance_null = 0.19363495614719278\n"
     ]
    }
   ],
   "source": [
    "# Calculate null deviance = total squared difference between the true output values and their mean\n",
    "# *Null model predicts every instance with the mean of the output variable\n",
    "y_test_mean = np.mean(y_test)\n",
    "deviance_null = np.sum((y_test - y_test_mean) ** 2) \n",
    "print(f\"{deviance_null = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e4c55da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.49947300777251824\n"
     ]
    }
   ],
   "source": [
    "# Calculate deviance-based R2\n",
    "R2 = 1 - (deviance / deviance_null)\n",
    "print(f\"{R2 = }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
